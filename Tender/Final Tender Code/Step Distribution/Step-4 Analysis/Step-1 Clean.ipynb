{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = r\"output_data.json\"\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient='records')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from cleantext import clean\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from typing import List, Set, Optional\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, language: str = 'english'):\n",
    "        \"\"\"\n",
    "        Initialize the text preprocessor with necessary NLTK downloads and configurations.\n",
    "        \n",
    "        Args:\n",
    "            language (str): Language for stopwords (default: 'english')\n",
    "        \"\"\"\n",
    "        # Download required NLTK resources\n",
    "        try:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: NLTK resource download failed: {e}\")\n",
    "        \n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.domain_stopwords = self._get_domain_stopwords()\n",
    "        \n",
    "    def _get_domain_stopwords(self) -> Set[str]:\n",
    "        \"\"\"Define domain-specific stopwords\"\"\"\n",
    "        return  {\n",
    "        'qty', 'nos', 'cm', 'mm', 'ps', 'set', 'technical', 'specification','provide','copy','right','total','rejected', \n",
    "        'section', 'vii', 'fdabiseuropean', 'u', 'x', 'l', 'b', 'c', 'd',\n",
    "        'e', 'f', 'h', 'i', 'j', 'k', 'l', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w',\n",
    "        'y', 'z', 'na', 'page', 'pageof', 'hz', 'ac', 'sr', 'general', 'point', 'department',\n",
    "        'neurosurgery', 'otorhinolaryngology', 'item', 'name', 'hr', 'year', 'unit', 'camc', 'item', \n",
    "        'specification', 'page', 'tender', 'jdh', 'qty', 'department', 'supply',\n",
    "        'installation', 'bid', 'aiims', 'jodhpur', 'rishikesh', 'delhi', 'raipur', 'one', \n",
    "        'following', 'procurement', 'services', 'contract', 'document', 'date', 'number', \n",
    "        'submission', 'address', 'financial', 'technical', 'criteria', 'validity', 'opening', \n",
    "        'closing', 'terms', 'conditions', 'authority', 'officer', 'manager', 'section', \n",
    "        'schedule', 'reference', 'project', 'quantity', 'value', 'requirement', 'agreement', \n",
    "        'proposal', 'evaluation', 'process', 'deadline', 'signature', 'quotation', 'form', \n",
    "        'office', 'contact', 'details', 'phone', 'email', 'fax', 'cost', 'price', 'information',\n",
    "        'admn', 'bidder', 'within', 'days', 'length', 'working', 'eprocure', 'gov', 'bidders', \n",
    "        'indian', 'institute', 'nos', 'cm', 'mm', 'ps', 'set', 'technical', 'specifications', \n",
    "        'section', 'vii', 'u', 'ce', 'x', 'l', 'b', 'c', 'd', 'e', 'f', 'h', 'th', 'tatibandh', 'g.e. road', \n",
    "        'cg', 'tele', 'website', 'no.', 'neuro', 'gte', \n",
    "        'i', 'j', 'k', 'l', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'na', 'least',\n",
    "        'page', 'pageof', 'hz', 'ac', 'sr', 'etc', 'may', 'either', 'general', 'points', 'aiims', \n",
    "        'would', 'etc', 'neurosurgery', 'a.', 'hrs', 'must', 'quoted', 'separately', \n",
    "        'otorhinolaryngology', 'required', 'name', 'hr', 'years', 'unit', 'camc', 'two', \n",
    "        'able', 'available', 'india', 'provided', 'patna', 'weather', 'without', 'shall', \n",
    "        'also', 'aiimsjdh', 'nagpur', 'bathinda', 'rish', 'inch', 'good', 'supplier', \n",
    "        'purchaser', 'aiimskalyani', 'kalyani', 'list', 'require', 'document', 'measurement', \n",
    "        'pagetender', 'along', 'bhubaneswar', 'bbsr', 'gem', 'per', 'time', 'odisha', \n",
    "        'annexure', 'year', 'aiimsmg', 'proc', 'gte', 'mangalagiri', 'goods', \n",
    "        'consignee', 'tenderer', 'period', 'bhopal', 'saket', 'nagar','aiims.jdh'\n",
    "    }\n",
    "    \n",
    "    def clean_text(self, text: Optional[str]) -> str:\n",
    "        \"\"\"\n",
    "        Clean and preprocess text for TF-IDF vectorization.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to clean\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned and preprocessed text\n",
    "        \"\"\"\n",
    "        # Handle None or empty input\n",
    "        if not text or pd.isna(text):\n",
    "            return ''\n",
    "        \n",
    "        try:\n",
    "            # Basic cleaning using cleantext\n",
    "            text = clean(\n",
    "                text.lower(),\n",
    "                fix_unicode=True,\n",
    "                to_ascii=True,\n",
    "                no_line_breaks=True,\n",
    "                no_urls=True,\n",
    "                no_emails=True,\n",
    "                no_phone_numbers=True,\n",
    "                no_numbers=True,\n",
    "                no_digits=True,\n",
    "                no_currency_symbols=True,\n",
    "                no_punct=False,\n",
    "                replace_with_url=\" \",\n",
    "                replace_with_email=\" \",\n",
    "                replace_with_phone_number=\" \",\n",
    "                replace_with_number=\" \",\n",
    "                replace_with_digit=\" \",\n",
    "                replace_with_currency_symbol=\" \",\n",
    "                lang=\"en\"\n",
    "            )\n",
    "            \n",
    "            # Replace special characters\n",
    "            text = text.replace('-', ' ').replace('/', ' ')\n",
    "            \n",
    "            # Handle newlines\n",
    "            text = re.sub(r'(\\b\\w{1,5})\\n(\\w{1,5}\\b)', r'\\1\\2', text)\n",
    "            text = re.sub(r'\\n+', ' ', text)\n",
    "            \n",
    "            # Remove numbers and non-alphabetic characters\n",
    "            text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "            \n",
    "            # Remove specific patterns\n",
    "            text = re.sub(r'\\b[ivxlcdm]+\\b', ' ', text)  # Roman numerals\n",
    "            \n",
    "            # Tokenize and clean words\n",
    "            words = [\n",
    "                self.lemmatizer.lemmatize(word)\n",
    "                for word in text.split()\n",
    "                if (word not in self.stop_words and \n",
    "                    word not in self.domain_stopwords and\n",
    "                    len(word) > 2)  # Remove very short words\n",
    "            ]\n",
    "            \n",
    "            return ' '.join(words)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing text: {e}\")\n",
    "            return ''\n",
    "    \n",
    "    def process_dataframe(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process multiple columns in a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            columns (List[str]): List of column names to process\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with processed text columns\n",
    "        \"\"\"\n",
    "        df_copy = df.copy()\n",
    "        for column in columns:\n",
    "            if column in df_copy.columns:\n",
    "                df_copy[column] = df_copy[column].apply(self.clean_text)\n",
    "        return df_copy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = TextPreprocessor()\n",
    "    \n",
    "    # Process DataFrame\n",
    "    columns_to_process = ['content', 'technical_specification']\n",
    "    processed_df = preprocessor.process_dataframe(df, columns_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from cleantext import clean\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import nltk\n",
    "# import pandas as pd  # Ensure pandas is imported\n",
    "\n",
    "# # Download the required resources from nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# # Initialize stopwords and lemmatizer\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def clean_text(text):\n",
    "#     # Check if the input is None\n",
    "#     if text is None:\n",
    "#         return ''  # Return an empty string or a placeholder\n",
    "\n",
    "#     # Convert to lowercase and clean the text\n",
    "#     text = clean(\n",
    "#         text.lower(),\n",
    "#         fix_unicode=True,\n",
    "#         to_ascii=True,\n",
    "#         no_line_breaks=True,\n",
    "#         no_urls=True,\n",
    "#         no_emails=True,\n",
    "#         no_phone_numbers=True,\n",
    "#         no_numbers=True,\n",
    "#         no_digits=True,\n",
    "#         no_currency_symbols=True,\n",
    "#         no_punct=False,  # Keep punctuation for later processing\n",
    "#         replace_with_url=\" \",\n",
    "#         replace_with_email=\" \",\n",
    "#         replace_with_phone_number=\" \",\n",
    "#         replace_with_number=\" \",\n",
    "#         replace_with_digit=\" \",\n",
    "#         replace_with_currency_symbol=\" \",\n",
    "#         lang=\"en\"\n",
    "#     )\n",
    "    \n",
    "#     # Replace hyphens and slashes with a space\n",
    "#     text = text.replace('-', ' ').replace('/', ' ')\n",
    "    \n",
    "# # Replace newlines between words without adding a space if both words have 5 or fewer characters\n",
    "#     text = re.sub(r'(\\b\\w{1,5})\\n(\\w{1,5}\\b)', r'\\1\\2', text)\n",
    "    \n",
    "#     # Replace remaining newlines with a space\n",
    "#     text = re.sub(r'\\n+', ' ', text)\n",
    "    \n",
    "#     # Remove numbers and punctuation/non-alphabetic characters\n",
    "#     text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "#     # Remove specific phrases and patterns\n",
    "#     text = re.sub(r'india\\s+institute\\s+medical\\s+science', ' ', text)\n",
    "#     text = re.sub(r'\\b[ivxlcdm]+\\b', ' ', text)  # Remove Roman numerals\n",
    "    \n",
    "#     # Tokenize the text\n",
    "#     words = text.split()\n",
    "    \n",
    "#     # Lemmatize and filter out both normal and domain-specific stopwords\n",
    "#     domain_stopwords = {\n",
    "#         'qty', 'nos', 'cm', 'mm', 'ps', 'set', 'technical', 'specification','provide','copy','right','total','rejected', \n",
    "#         'section', 'vii', 'fdabiseuropean', 'u', 'x', 'l', 'b', 'c', 'd',\n",
    "#         'e', 'f', 'h', 'i', 'j', 'k', 'l', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w',\n",
    "#         'y', 'z', 'na', 'page', 'pageof', 'hz', 'ac', 'sr', 'general', 'point', 'department',\n",
    "#         'neurosurgery', 'otorhinolaryngology', 'item', 'name', 'hr', 'year', 'unit', 'camc', 'item', \n",
    "#         'specification', 'page', 'tender', 'jdh', 'qty', 'department', 'supply',\n",
    "#         'installation', 'bid', 'aiims', 'jodhpur', 'rishikesh', 'delhi', 'raipur', 'one', \n",
    "#         'following', 'procurement', 'services', 'contract', 'document', 'date', 'number', \n",
    "#         'submission', 'address', 'financial', 'technical', 'criteria', 'validity', 'opening', \n",
    "#         'closing', 'terms', 'conditions', 'authority', 'officer', 'manager', 'section', \n",
    "#         'schedule', 'reference', 'project', 'quantity', 'value', 'requirement', 'agreement', \n",
    "#         'proposal', 'evaluation', 'process', 'deadline', 'signature', 'quotation', 'form', \n",
    "#         'office', 'contact', 'details', 'phone', 'email', 'fax', 'cost', 'price', 'information',\n",
    "#         'admn', 'bidder', 'within', 'days', 'length', 'working', 'eprocure', 'gov', 'bidders', \n",
    "#         'indian', 'institute', 'nos', 'cm', 'mm', 'ps', 'set', 'technical', 'specifications', \n",
    "#         'section', 'vii', 'u', 'ce', 'x', 'l', 'b', 'c', 'd', 'e', 'f', 'h', 'th', 'tatibandh', 'g.e. road', \n",
    "#         'cg', 'tele', 'website', 'no.', 'neuro', 'gte', \n",
    "#         'i', 'j', 'k', 'l', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'na', 'least',\n",
    "#         'page', 'pageof', 'hz', 'ac', 'sr', 'etc', 'may', 'either', 'general', 'points', 'aiims', \n",
    "#         'would', 'etc', 'neurosurgery', 'a.', 'hrs', 'must', 'quoted', 'separately', \n",
    "#         'otorhinolaryngology', 'required', 'name', 'hr', 'years', 'unit', 'camc', 'two', \n",
    "#         'able', 'available', 'india', 'provided', 'patna', 'weather', 'without', 'shall', \n",
    "#         'also', 'aiimsjdh', 'nagpur', 'bathinda', 'rish', 'inch', 'good', 'supplier', \n",
    "#         'purchaser', 'aiimskalyani', 'kalyani', 'list', 'require', 'document', 'measurement', \n",
    "#         'pagetender', 'along', 'bhubaneswar', 'bbsr', 'gem', 'per', 'time', 'odisha', \n",
    "#         'annexure', 'year', 'aiimsmg', 'proc', 'gte', 'mangalagiri', 'goods', \n",
    "#         'consignee', 'tenderer', 'period', 'bhopal', 'saket', 'nagar','aiims.jdh'\n",
    "#     }\n",
    "    \n",
    "#     # Filter out normal stopwords and domain-specific stopwords\n",
    "#     cleaned_words = [\n",
    "#         lemmatizer.lemmatize(word) \n",
    "#         for word in words \n",
    "#         if word not in stop_words and word not in domain_stopwords\n",
    "#     ]\n",
    "    \n",
    "#     # Join the cleaned words back into a string\n",
    "#     cleaned_text = ' '.join(cleaned_words)\n",
    "    \n",
    "#     # Return the cleaned text\n",
    "#     return cleaned_text\n",
    "\n",
    "# # Apply the function to the 'Technical Specification' column in the DataFrame\n",
    "# df.loc[:, 'content'] = df['content'].apply(clean_text)\n",
    "# df.loc[:, 'technical_specification'] = df['technical_specification'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_json('cleaned_files.json', orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_html('check.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
