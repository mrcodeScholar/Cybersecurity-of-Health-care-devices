{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS BIBINAGAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--headless')  # Run in headless mode, remove this line if you want to see the browser window\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scroll to the bottom of the page to handle infinite scroll\n",
    "# def scroll_to_bottom(driver):\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     while True:\n",
    "#         # Scroll down to the bottom\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         # Wait for new content to load\n",
    "#         time.sleep(3)  # Adjust this depending on the page's load time\n",
    "#         # Check if the scroll height has changed\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             break\n",
    "#         last_height = new_height\n",
    "\n",
    "# # Scrape the table content, including extracting links from the \"Tender Title\" column\n",
    "# def scrape_table_data(driver):\n",
    "#     table_data = []\n",
    "    \n",
    "#     # Locate the table rows, wait for table to load\n",
    "#     try:\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.XPATH, \"//table/tbody/tr\"))\n",
    "#         )\n",
    "#     except TimeoutException:\n",
    "#         print(\"Table did not load in time.\")\n",
    "#         return table_data\n",
    "\n",
    "#     rows = driver.find_elements(By.XPATH, \"//table/tbody/tr\")\n",
    "\n",
    "#     for row in rows:\n",
    "#         # Get all the cells (columns) in the row\n",
    "#         cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        \n",
    "#         # Ensure the row has data\n",
    "#         if len(cols) > 0:\n",
    "#             row_data = []\n",
    "#             for i, col in enumerate(cols):\n",
    "#                 # Check if we're in the \"Tender Title\" column (assumed to be column 1 or index 0)\n",
    "#                 if i == 2:  # Assuming Tender Title is the first column, adjust index if different\n",
    "#                     try:\n",
    "#                         # Try to extract the link if there's an anchor tag inside\n",
    "#                         link_elem = col.find_element(By.TAG_NAME, \"a\")\n",
    "#                         title = link_elem.text\n",
    "#                         link = link_elem.get_attribute(\"href\")\n",
    "#                     except NoSuchElementException:\n",
    "#                         # If no link is found, just grab the text and leave the link blank\n",
    "#                         title = col.text\n",
    "#                         link = \"\"\n",
    "                    \n",
    "#                     row_data.append(title)  # Append the title text\n",
    "#                     row_data.append(link)   # Append the link in a new column\n",
    "#                 else:\n",
    "#                     row_data.append(col.text)  # Append the normal text for other columns\n",
    "#             table_data.append(row_data)\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_aiims_tender():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Open the URL\n",
    "#     driver.get(\"https://aiimsbibinagar.edu.in/tender.html\")\n",
    "    \n",
    "#     # Scroll to the bottom to ensure all content is loaded (if infinite scroll exists)\n",
    "#     scroll_to_bottom(driver)\n",
    "    \n",
    "#     # Scrape the table\n",
    "#     table_data = scrape_table_data(driver)\n",
    "    \n",
    "#     # Automatically detect the number of columns in the first row\n",
    "#     if len(table_data) > 0:\n",
    "#         num_columns = len(table_data[0])  # Get the number of columns from the first row\n",
    "    \n",
    "#         # Create generic column headers and add one for \"Tender Link\"\n",
    "#         column_headers = ['Tender Title', 'Tender Link'] + [f\"Column {i+2}\" for i in range(num_columns - 2)]\n",
    "    \n",
    "#         # Convert the data into a pandas DataFrame\n",
    "#         df = pd.DataFrame(table_data, columns=column_headers)\n",
    "    \n",
    "#         # Save the DataFrame to an Excel file\n",
    "#         df.to_excel(\"aiims_tender_data_with_links.xlsx\", index=False)\n",
    "    \n",
    "#         print(f\"Data has been successfully scraped and saved to 'aiims_tender_data_with_links.xlsx'.\")\n",
    "    \n",
    "#     else:\n",
    "#         print(\"No data found in the table.\")\n",
    "    \n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_aiims_tender()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS RAJKOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import time\n",
    "\n",
    "# # Initialize the WebDriver\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# # Load the website\n",
    "# driver.get(\"https://aiimsrajkot.edu.in/tendernew\")\n",
    "\n",
    "# # Wait for the page to load and the table to be visible\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# # Step 1: Change the number of rows to 100\n",
    "# select_length = wait.until(EC.element_to_be_clickable((By.XPATH, '/html/body/div/div[4]/div/div[2]/div/div/div/div/article/div/div/div[2]/div[1]/label/select')))\n",
    "# select_length.click()\n",
    "# time.sleep(1)  # Let the dropdown animation happen\n",
    "# select_length.send_keys(Keys.ARROW_DOWN)  # Move down to 100\n",
    "# select_length.send_keys(Keys.ARROW_DOWN)  # Adjust as per need (depending on position of 100)\n",
    "# select_length.send_keys(Keys.ARROW_DOWN)  # Adjust as per need (depending on position of 100)\n",
    "\n",
    "# select_length.send_keys(Keys.ENTER)\n",
    "\n",
    "# # Wait for table to reload with 100 rows\n",
    "# time.sleep(3)\n",
    "\n",
    "# # Step 2: Scrape the table rows\n",
    "# table_rows = driver.find_elements(By.XPATH, '//table[@id=\"tableID\"]/tbody/tr')\n",
    "\n",
    "# # Create a list to store scraped data\n",
    "# scraped_data = []\n",
    "\n",
    "# # Iterate over each row and extract the needed columns\n",
    "# for row in table_rows:\n",
    "#     # Tender column\n",
    "#     tender = row.find_element(By.XPATH, './th[2]').text.strip()\n",
    "\n",
    "#     # Open Date column\n",
    "#     open_date = row.find_element(By.XPATH, './td[1]').text.strip()\n",
    "\n",
    "#     # Try to get the Document link (PDF)\n",
    "#     try:\n",
    "#         # Locate the anchor tag inside any paragraph within the document cell\n",
    "#         document_link = row.find_element(By.XPATH, './td[3]//a').get_attribute('href')\n",
    "#     except:\n",
    "#         document_link = \"No document link\"\n",
    "\n",
    "#     # Append the extracted data to the list\n",
    "#     scraped_data.append({\n",
    "#         'Tender': tender,\n",
    "#         'Open Date': open_date,\n",
    "#         'Document Link': document_link\n",
    "#     })\n",
    "\n",
    "# # Step 3: Print or save the scraped data\n",
    "# for entry in scraped_data:\n",
    "#     print(entry)\n",
    "\n",
    "# # Close the driver\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS BILASPUR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # Initialize the WebDriver (assuming Chrome, but you can use any)\n",
    "# driver = webdriver.Chrome()  # Ensure chromedriver is in your PATH\n",
    "\n",
    "# # Navigate to the page\n",
    "# url = \"https://www.aiimsbilaspur.edu.in/tenders\"\n",
    "# driver.get(url)\n",
    "\n",
    "# # Wait for the page to load fully\n",
    "# time.sleep(3)\n",
    "\n",
    "# # Get the page source\n",
    "# html = driver.page_source\n",
    "\n",
    "# # Close the browser\n",
    "# driver.quit()\n",
    "\n",
    "# # Parse the page source with BeautifulSoup\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# # Find all divs with class \"recruitment_table\"\n",
    "# recruitment_tables = soup.find_all('div', class_='recruitment_table')\n",
    "\n",
    "# # Initialize a list to store the scraped data\n",
    "# data = []\n",
    "\n",
    "# # Extract data from each \"recruitment_table\" div\n",
    "# for table in recruitment_tables:\n",
    "#     title_element = table.find('h4')\n",
    "#     date_element = table.find('p', class_='file_date')\n",
    "#     download_link_element = table.find('a', class_='download_btn_recruitment')\n",
    "#     view_link_elements = table.find_all('a', class_='download_btn_recruitment')\n",
    "\n",
    "#     if title_element and date_element and download_link_element and len(view_link_elements) > 1:\n",
    "#         title = title_element.get_text(strip=True)\n",
    "#         date = date_element.get_text(strip=True).replace('fa-calendar', '').strip()  # Remove the icon part\n",
    "#         download_link = download_link_element['href']\n",
    "#         view_link = view_link_elements[1]['href']\n",
    "\n",
    "#         # Append the extracted data to the list\n",
    "#         data.append({\n",
    "#             'Title': title,\n",
    "#             'Date': date,\n",
    "#             'Download Link': download_link,\n",
    "#             'View Link': view_link\n",
    "#         })\n",
    "#     else:\n",
    "#         # Print a warning if some elements are missing\n",
    "#         print(f\"Warning: Skipping a table due to missing data. Table HTML: {table}\")\n",
    "\n",
    "# # Create a DataFrame using pandas\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Save the DataFrame to an Excel file\n",
    "# df.to_excel('aiims_tenders.xlsx', index=False)\n",
    "\n",
    "# print(\"Data scraped and saved to aiims_tenders.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS BHATINDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException, TimeoutException\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     options.add_argument('--headless')  # Optional: Run headless if you don't want to see the browser\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Function to remove illegal characters for Excel\n",
    "# def clean_text(text):\n",
    "#     # Define a regex pattern to remove control characters\n",
    "#     # This will remove characters not allowed in Excel (like ASCII control codes)\n",
    "#     illegal_characters_pattern = r\"[\\x00-\\x1F\\x7F-\\x9F]\"\n",
    "#     return re.sub(illegal_characters_pattern, \"\", text)\n",
    "\n",
    "# # Scrape the table content, including extracting links from the \"Related Links\" column\n",
    "# def scrape_table_data(driver):\n",
    "#     table_data = []\n",
    "    \n",
    "#     # Locate the table body containing the rows\n",
    "#     table_body = driver.find_element(By.CSS_SELECTOR, \"#DataTables_Table_0 tbody\")\n",
    "#     rows = table_body.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "#     for row in rows:\n",
    "#         # Get all the cells (columns) in the row\n",
    "#         cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        \n",
    "#         # Ensure the row has data\n",
    "#         if len(cols) > 0:\n",
    "#             # Extract the data for each column and clean illegal characters\n",
    "#             tender_id = clean_text(cols[0].text)\n",
    "#             tender_ref = clean_text(cols[1].text)\n",
    "#             title = clean_text(cols[2].text)\n",
    "#             start_date = clean_text(cols[3].text)\n",
    "#             end_date = clean_text(cols[4].text)\n",
    "            \n",
    "#             # Extract the link from the \"Related Links\" column\n",
    "#             try:\n",
    "#                 link_element = cols[5].find_element(By.TAG_NAME, \"a\")\n",
    "#                 link_text = clean_text(link_element.text)\n",
    "#                 link_url = link_element.get_attribute(\"href\")\n",
    "#             except NoSuchElementException:\n",
    "#                 link_text = \"\"\n",
    "#                 link_url = \"\"\n",
    "\n",
    "#             # Print the scraped values for tracking\n",
    "#             print(f\"Scraped: {tender_id}, {tender_ref}, {title}, {start_date}, {end_date}, {link_text}, {link_url}\")\n",
    "            \n",
    "#             # Append the extracted data to table_data\n",
    "#             table_data.append({\n",
    "#                 \"Tender ID\": tender_id,\n",
    "#                 \"Tender Ref.\": tender_ref,\n",
    "#                 \"Title\": title,\n",
    "#                 \"Start Date\": start_date,\n",
    "#                 \"End Date\": end_date,\n",
    "#                 \"Related Link Text\": link_text,\n",
    "#                 \"Related Link URL\": link_url\n",
    "#             })\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# # Function to handle pagination and scrape multiple pages\n",
    "# def scrape_paginated_data(driver):\n",
    "#     all_table_data = []  # To store data from all pages\n",
    "#     page_count = 1  # Counter for pages, useful for debugging\n",
    "\n",
    "#     while True:\n",
    "#         print(f\"Scraping page {page_count}...\")\n",
    "\n",
    "#         # Scrape the data from the current page\n",
    "#         table_data = scrape_table_data(driver)\n",
    "#         all_table_data.extend(table_data)\n",
    "\n",
    "#         try:\n",
    "#             # Locate the \"Next\" button using XPath\n",
    "#             next_button = WebDriverWait(driver, 10).until(\n",
    "#                 EC.presence_of_element_located((By.XPATH, \"//*[@id='DataTables_Table_0_next']/a\"))\n",
    "#             )\n",
    "            \n",
    "#             # Check if the \"Next\" button is disabled (i.e., last page)\n",
    "#             if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "#                 print(\"Reached the last page.\")\n",
    "#                 break\n",
    "            \n",
    "#             # Scroll into view and click the \"Next\" button\n",
    "#             driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "#             next_button.click()\n",
    "\n",
    "#             # Wait for the next page to load by waiting for the table to update\n",
    "#             WebDriverWait(driver, 10).until(\n",
    "#                 EC.staleness_of(next_button)  # Wait for the \"Next\" button to become stale (i.e., page reloads)\n",
    "#             )\n",
    "            \n",
    "#             # Increment page count for tracking\n",
    "#             page_count += 1\n",
    "\n",
    "#         except (NoSuchElementException, TimeoutException):\n",
    "#             # If the \"Next\" button is not found or not clickable, break the loop (end of pagination)\n",
    "#             print(\"Reached the last page or no 'Next' button found.\")\n",
    "#             break\n",
    "\n",
    "#         except ElementNotInteractableException:\n",
    "#             # If the \"Next\" button is disabled or not interactable, break the loop\n",
    "#             print(\"No more pages to scrape or 'Next' button is not interactable.\")\n",
    "#             break\n",
    "\n",
    "#     return all_table_data\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_aiims_tender():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Open the URL\n",
    "#     driver.get(\"https://aiimsbathinda.edu.in/Procurements.aspx?FXoLDJ=ydRRTTxkdt6Trx91pX1+cA==&JnH7tY=ni6p7uJmUYh6NtRIoH1zej6ayg0V9vON2iB6aRQlYRo=\")\n",
    "\n",
    "#     # Scrape the paginated data\n",
    "#     all_table_data = scrape_paginated_data(driver)\n",
    "    \n",
    "#     # Create a DataFrame using pandas\n",
    "#     if all_table_data:\n",
    "#         df = pd.DataFrame(all_table_data)\n",
    "    \n",
    "#         # Save the DataFrame to an Excel file\n",
    "#         df.to_excel(\"aiims_bathinda_procurements.xlsx\", index=False)\n",
    "    \n",
    "#         print(f\"Data has been successfully scraped and saved to 'aiims_bathinda_procurements.xlsx'.\")\n",
    "#     else:\n",
    "#         print(\"No data found in the table.\")\n",
    "    \n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_aiims_tender()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS KALYANI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--headless')  # Run in headless mode, remove this line if you want to see the browser window\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scroll to the bottom of the page to handle infinite scroll\n",
    "# def scroll_to_bottom(driver):\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     while True:\n",
    "#         # Scroll down to the bottom\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         # Wait for new content to load\n",
    "#         time.sleep(3)  # Adjust this depending on the page's load time\n",
    "#         # Check if the scroll height has changed\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             break\n",
    "#         last_height = new_height\n",
    "\n",
    "# # Scrape the table content, including extracting links from the \"Tender Title\" column\n",
    "# def scrape_table_data(driver):\n",
    "#     table_data = []\n",
    "    \n",
    "#     # Locate the table rows, wait for table to load\n",
    "#     try:\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.XPATH, \"//table/tbody/tr\"))\n",
    "#         )\n",
    "#     except TimeoutException:\n",
    "#         print(\"Table did not load in time.\")\n",
    "#         return table_data\n",
    "\n",
    "#     rows = driver.find_elements(By.XPATH, \"//table/tbody/tr\")\n",
    "\n",
    "#     for row in rows:\n",
    "#         # Get all the cells (columns) in the row\n",
    "#         cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        \n",
    "#         # Ensure the row has data\n",
    "#         if len(cols) > 0:\n",
    "#             row_data = []\n",
    "#             for i, col in enumerate(cols):\n",
    "#                 # Check if we're in the \"Tender Title\" column (assumed to be column 1 or index 0)\n",
    "#                 if i == 2:  # Assuming Tender Title is the first column, adjust index if different\n",
    "#                     try:\n",
    "#                         # Try to extract the link if there's an anchor tag inside\n",
    "#                         link_elem = col.find_element(By.TAG_NAME, \"a\")\n",
    "#                         title = link_elem.text\n",
    "#                         link = link_elem.get_attribute(\"href\")\n",
    "#                     except NoSuchElementException:\n",
    "#                         # If no link is found, just grab the text and leave the link blank\n",
    "#                         title = col.text\n",
    "#                         link = \"\"\n",
    "                    \n",
    "#                     row_data.append(title)  # Append the title text\n",
    "#                     row_data.append(link)   # Append the link in a new column\n",
    "#                 else:\n",
    "#                     row_data.append(col.text)  # Append the normal text for other columns\n",
    "#             table_data.append(row_data)\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_aiims_tender():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Open the URL\n",
    "#     driver.get(\"https://aiimskalyani.edu.in/aiims-k-e-tender/\")\n",
    "    \n",
    "#     # Scroll to the bottom to ensure all content is loaded (if infinite scroll exists)\n",
    "#     scroll_to_bottom(driver)\n",
    "    \n",
    "#     # Scrape the table\n",
    "#     table_data = scrape_table_data(driver)\n",
    "    \n",
    "#     # Automatically detect the number of columns in the first row\n",
    "#     if len(table_data) > 0:\n",
    "#         num_columns = len(table_data[0])  # Get the number of columns from the first row\n",
    "    \n",
    "#         # Create generic column headers and add one for \"Tender Link\"\n",
    "#         column_headers = ['Tender Title', 'Tender Link'] + [f\"Column {i+2}\" for i in range(num_columns - 2)]\n",
    "    \n",
    "#         # Convert the data into a pandas DataFrame\n",
    "#         df = pd.DataFrame(table_data, columns=column_headers)\n",
    "    \n",
    "#         # Save the DataFrame to an Excel file\n",
    "#         df.to_excel(\"aiims_kalyani.xlsx\", index=False)\n",
    "    \n",
    "#         print(f\"Data has been successfully scraped and saved '.\")\n",
    "    \n",
    "#     else:\n",
    "#         print(\"No data found in the table.\")\n",
    "    \n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_aiims_tender()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS GORKHPUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scrape additional tender information from blog_ref div\n",
    "# def scrape_blog_ref_tenders(driver):\n",
    "#     tenders_data = []\n",
    "    \n",
    "#     try:\n",
    "#         # Wait until the specific div class blog_ref is present\n",
    "#         blog_refs = WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_all_elements_located((By.CLASS_NAME, \"blog_ref\"))\n",
    "#         )\n",
    "        \n",
    "#         for blog_ref in blog_refs:\n",
    "#             try:\n",
    "#                 # Extract title from the h4 tag\n",
    "#                 title = blog_ref.find_element(By.TAG_NAME, \"h4\").text\n",
    "                \n",
    "#                 # Extract the href from the a tag inside the blockquote\n",
    "#                 link_element = blog_ref.find_element(By.TAG_NAME, \"a\")\n",
    "#                 link = link_element.get_attribute(\"href\")\n",
    "#                 link_text = link_element.text\n",
    "                \n",
    "#                 # Print the extracted data for verification\n",
    "#                 print(f\"Scraped Blog Ref: Title: {title}, Link Text: {link_text}, Link: {link}\")\n",
    "                \n",
    "#                 # Append data to list\n",
    "#                 tenders_data.append({\n",
    "#                     \"Title\": title,\n",
    "#                     \"Link Text\": link_text,\n",
    "#                     \"Link\": link\n",
    "#                 })\n",
    "#             except NoSuchElementException:\n",
    "#                 print(\"Could not locate some elements within blog_ref\")\n",
    "    \n",
    "#     except TimeoutException:\n",
    "#         print(\"Timed out waiting for blog_ref elements.\")\n",
    "    \n",
    "#     return tenders_data\n",
    "\n",
    "# # Scrape the table content, including extracting links from the \"Title\" column\n",
    "# def scrape_table_data(driver):\n",
    "#     table_data = []\n",
    "\n",
    "#     try:\n",
    "#         # Locate the table body containing the rows, wait until it's available\n",
    "#         table_body = WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.CSS_SELECTOR, \"#example tbody\"))\n",
    "#         )\n",
    "        \n",
    "#         rows = table_body.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "#         for row in rows:\n",
    "#             # Get all the cells (columns) in the row\n",
    "#             cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            \n",
    "#             # Ensure the row has data\n",
    "#             if len(cols) > 0:\n",
    "#                 # Extract Title and the embedded link\n",
    "#                 try:\n",
    "#                     title_element = cols[0].find_element(By.TAG_NAME, \"a\")\n",
    "#                     title_text = title_element.text\n",
    "#                     title_link = title_element.get_attribute(\"href\")\n",
    "#                 except NoSuchElementException:\n",
    "#                     title_text = \"\"\n",
    "#                     title_link = \"\"\n",
    "\n",
    "#                 # Extract Start Date and End Date\n",
    "#                 start_date = cols[1].text\n",
    "#                 end_date = cols[2].text\n",
    "                \n",
    "#                 # Print the scraped values for tracking\n",
    "#                 print(f\"Scraped: Title: {title_text}, Start Date: {start_date}, End Date: {end_date}, Link: {title_link}\")\n",
    "                \n",
    "#                 # Append the extracted data to table_data\n",
    "#                 table_data.append({\n",
    "#                     \"Title\": title_text,\n",
    "#                     \"Start Date\": start_date,\n",
    "#                     \"End Date\": end_date,\n",
    "#                     \"Link\": title_link\n",
    "#                 })\n",
    "    \n",
    "#     except TimeoutException:\n",
    "#         print(\"Timed out waiting for table to load.\")\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_aiims_tenders():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Open the URL\n",
    "#     driver.get(\"https://aiimsgorakhpur.edu.in/tenders/\")\n",
    "    \n",
    "#     # Give the page time to load\n",
    "#     time.sleep(5)  # This can be optimized with explicit waits\n",
    "\n",
    "#     # Scrape the table data\n",
    "#     all_table_data = scrape_table_data(driver)\n",
    "    \n",
    "#     # Scrape blog_ref tenders\n",
    "#     blog_ref_tenders = scrape_blog_ref_tenders(driver)\n",
    "    \n",
    "#     # Combine all data\n",
    "#     if all_table_data or blog_ref_tenders:\n",
    "#         combined_data = all_table_data + blog_ref_tenders\n",
    "\n",
    "#         # Create a DataFrame using pandas\n",
    "#         df = pd.DataFrame(combined_data)\n",
    "    \n",
    "#         # Save the DataFrame to an Excel file\n",
    "#         df.to_excel(\"aiims_gorakhpur_tenders_with_blog_ref.xlsx\", index=False)\n",
    "    \n",
    "#         print(f\"Data has been successfully scraped and saved to 'aiims_gorakhpur_tenders_with_blog_ref.xlsx'.\")\n",
    "#     else:\n",
    "#         print(\"No data found.\")\n",
    "    \n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_aiims_tenders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS NAGPUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scrape the main table content from the current tenders page\n",
    "# def scrape_main_table(driver):\n",
    "#     table_data = []\n",
    "\n",
    "#     try:\n",
    "#         # Wait until the table is present\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.CSS_SELECTOR, \"#current_tender_id\"))\n",
    "#         )\n",
    "\n",
    "#         # Locate the table body containing the rows\n",
    "#         table_body = driver.find_element(By.CSS_SELECTOR, \"#current_tender_id tbody\")\n",
    "#         rows = table_body.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "#         for row in rows:\n",
    "#             # Get all the cells (columns) in the row\n",
    "#             cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            \n",
    "#             # Ensure the row has data\n",
    "#             if len(cols) > 0:\n",
    "#                 # Extract Title and the embedded link in the first column\n",
    "#                 try:\n",
    "#                     title_element = cols[1].find_element(By.TAG_NAME, \"a\")\n",
    "#                     title_text = title_element.text\n",
    "#                     title_link = title_element.get_attribute(\"href\")\n",
    "#                 except NoSuchElementException:\n",
    "#                     title_text = \"\"\n",
    "#                     title_link = \"\"\n",
    "\n",
    "#                 # Extract Ref. No., Publish Date, and Last Submission Date\n",
    "#                 ref_no = cols[2].text\n",
    "#                 publish_date = cols[3].text\n",
    "#                 submission_date = cols[4].text\n",
    "                \n",
    "#                 # Print the scraped values for tracking\n",
    "#                 print(f\"Scraped: Title: {title_text}, Ref No: {ref_no}, Publish Date: {publish_date}, Last Submission Date: {submission_date}, Link: {title_link}\")\n",
    "                \n",
    "#                 # Append the extracted data to table_data\n",
    "#                 table_data.append({\n",
    "#                     \"Title\": title_text,\n",
    "#                     \"Ref No\": ref_no,\n",
    "#                     \"Publish Date\": publish_date,\n",
    "#                     \"Last Submission Date\": submission_date,\n",
    "#                     \"Link\": title_link\n",
    "#                 })\n",
    "\n",
    "#     except TimeoutException:\n",
    "#         print(\"Error: Timed out waiting for the table to load.\")\n",
    "#     except NoSuchElementException:\n",
    "#         print(\"Error: Could not find the table on the page.\")\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# # Scrape only the href value of the download link from the \"Attachments\" section\n",
    "# def scrape_attachments(driver):\n",
    "#     download_links = []\n",
    "\n",
    "#     try:\n",
    "#         # Locate the attachments section by looking for <h4> with text \"Attachments\"\n",
    "#         details_box = driver.find_element(By.XPATH, \"//div[@class='details_box']//h4[text()='Attachments']/following-sibling::div[contains(@class, 'table-responsive')]//tbody\")\n",
    "#         rows = details_box.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "#         for row in rows:\n",
    "#             # Find all columns in the row\n",
    "#             cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "\n",
    "#             # Check if the row contains the expected number of columns\n",
    "#             if len(cols) >= 3:\n",
    "#                 try:\n",
    "#                     # Find the <a> tag and extract the href attribute\n",
    "#                     download_link = cols[2].find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                    \n",
    "#                     # Handle relative links by appending the base URL if necessary\n",
    "#                     if not download_link.startswith(\"http\"):\n",
    "#                         download_link = f\"https://aiimsnagpur.edu.in{download_link}\"\n",
    "                    \n",
    "#                     # Append the link to the download_links list\n",
    "#                     download_links.append(download_link)\n",
    "                \n",
    "#                 except NoSuchElementException:\n",
    "#                     print(\"No download link found in this row.\")\n",
    "#             else:\n",
    "#                 print(f\"Skipping row with insufficient columns: {len(cols)} columns found.\")\n",
    "\n",
    "#     except NoSuchElementException:\n",
    "#         print(\"No 'Attachments' section found on this page.\")\n",
    "\n",
    "#     return download_links\n",
    "\n",
    "# # Re-select '100 entries' on the main page\n",
    "# def select_100_entries(driver):\n",
    "#     try:\n",
    "#         # Wait for the dropdown to be available and select '100 entries'\n",
    "#         time.sleep(3)  # Give the page time to load\n",
    "#         select_element = Select(driver.find_element(By.NAME, \"current_tender_id_length\"))\n",
    "#         select_element.select_by_value(\"100\")\n",
    "#         time.sleep(3)  # Give the page time to refresh\n",
    "#     except NoSuchElementException:\n",
    "#         print(\"Error: Could not find the '100 entries' dropdown.\")\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_aiims_nagpur_tenders():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Open the URL\n",
    "#     driver.get(\"https://aiimsnagpur.edu.in/tendersfront/current_tenders\")\n",
    "    \n",
    "#     # Select '100 entries' from the dropdown if necessary\n",
    "#     select_100_entries(driver)\n",
    "\n",
    "#     # Scrape the main table data\n",
    "#     all_table_data = scrape_main_table(driver)\n",
    "\n",
    "#     # Iterate through each tender to scrape the download links\n",
    "#     for tender in all_table_data:\n",
    "#         print(f\"Navigating to: {tender['Title']}\")\n",
    "\n",
    "#         # Check if the link is relative or absolute\n",
    "#         if not tender['Link'].startswith(\"http\"):\n",
    "#             tender_url = f\"https://aiimsnagpur.edu.in{tender['Link']}\"\n",
    "#         else:\n",
    "#             tender_url = tender['Link']\n",
    "\n",
    "#         # Print the full URL for debugging purposes\n",
    "#         print(f\"Navigating to URL: {tender_url}\")\n",
    "        \n",
    "#         # Navigate to the tender link\n",
    "#         driver.get(tender_url)\n",
    "\n",
    "#         # Scrape the attachments from the details page\n",
    "#         attachments = scrape_attachments(driver)\n",
    "#         tender[\"Attachments\"] = attachments  # Add the attachments to the tender data\n",
    "        \n",
    "#         # Print the scraped attachments for this tender\n",
    "#         print(f\"Attachments for {tender['Title']}: {attachments}\")\n",
    "\n",
    "#         # Navigate back to the main tenders page\n",
    "#         driver.back()\n",
    "        \n",
    "#         # Re-select '100 entries' on the main page again after navigating back\n",
    "#         select_100_entries(driver)\n",
    "\n",
    "#     # Create a DataFrame using pandas\n",
    "#     if all_table_data:\n",
    "#         df = pd.DataFrame(all_table_data)\n",
    "    \n",
    "#         # Save the DataFrame to an Excel file\n",
    "#         df.to_excel(\"aiims_nagpur_tenders.xlsx\", index=False)\n",
    "    \n",
    "#         print(f\"Data has been successfully scraped and saved to 'aiims_nagpur_tenders.xlsx'.\")\n",
    "#     else:\n",
    "#         print(\"No data found in the table.\")\n",
    "    \n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_aiims_nagpur_tenders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS NAGPUR ARCHIVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scrape the main table content from the current tenders page\n",
    "# def scrape_main_table(driver):\n",
    "#     table_data = []\n",
    "\n",
    "#     try:\n",
    "#         # Wait until the table is present\n",
    "#         WebDriverWait(driver, 15).until(\n",
    "#             EC.presence_of_element_located((By.ID, \"archieved_tender_id\"))\n",
    "#         )\n",
    "\n",
    "#         # Locate the table body containing the rows\n",
    "#         table_body = driver.find_element(By.CSS_SELECTOR, \"#archieved_tender_id tbody\")\n",
    "#         rows = table_body.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "#         for row in rows:\n",
    "#             # Get all the cells (columns) in the row\n",
    "#             cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            \n",
    "#             # Ensure the row has data\n",
    "#             if len(cols) > 0:\n",
    "#                 # Extract Title and the embedded link in the second column\n",
    "#                 try:\n",
    "#                     title_element = cols[1].find_element(By.TAG_NAME, \"a\")\n",
    "#                     title_text = title_element.text\n",
    "#                     title_link = title_element.get_attribute(\"href\")\n",
    "#                 except NoSuchElementException:\n",
    "#                     title_text = \"\"\n",
    "#                     title_link = \"\"\n",
    "\n",
    "#                 # Extract Ref. No., and Last Submission Date\n",
    "#                 ref_no = cols[0].text  # Assuming ref_no is the Sr number\n",
    "#                 submission_date = cols[3].text  # Last Submission Date in the fourth column\n",
    "                \n",
    "#                 # Click the title link to open the details page and scrape attachments\n",
    "#                 if title_link:\n",
    "#                     driver.execute_script(\"window.open('');\")  # Open a new tab\n",
    "#                     driver.switch_to.window(driver.window_handles[1])  # Switch to new tab\n",
    "#                     driver.get(title_link)  # Go to the details page\n",
    "                    \n",
    "#                     # Scrape attachments\n",
    "#                     attachments = scrape_attachments(driver)\n",
    "                    \n",
    "#                     # Close the details tab and switch back to the main page\n",
    "#                     driver.close()\n",
    "#                     driver.switch_to.window(driver.window_handles[0])\n",
    "#                 else:\n",
    "#                     attachments = []\n",
    "\n",
    "#                 # Print the scraped values for tracking\n",
    "#                 print(f\"Scraped: Title: {title_text}, Ref No: {ref_no}, Last Submission Date: {submission_date}, Attachments: {attachments}\")\n",
    "                \n",
    "#                 # Append the extracted data to table_data\n",
    "#                 table_data.append({\n",
    "#                     \"Title\": title_text,\n",
    "#                     \"Ref No\": ref_no,\n",
    "#                     \"Last Submission Date\": submission_date,\n",
    "#                     \"Link\": title_link,\n",
    "#                     \"Attachments\": attachments\n",
    "#                 })\n",
    "\n",
    "#     except TimeoutException:\n",
    "#         print(\"Error: Timed out waiting for the table to load.\")\n",
    "#     except NoSuchElementException:\n",
    "#         print(\"Error: Could not find the table on the page.\")\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# # Scrape only the href value of the download link from the \"Attachments\" section\n",
    "# def scrape_attachments(driver):\n",
    "#     download_links = []\n",
    "\n",
    "#     try:\n",
    "#         # Locate the attachments section by looking for <h4> with text \"Attachments\"\n",
    "#         details_box = driver.find_element(By.XPATH, \"//div[@class='details_box']//h4[text()='Attachments']/following-sibling::div[contains(@class, 'table-responsive')]//tbody\")\n",
    "#         rows = details_box.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "#         for row in rows:\n",
    "#             # Find all columns in the row\n",
    "#             cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "\n",
    "#             # Check if the row contains the expected number of columns\n",
    "#             if len(cols) >= 3:\n",
    "#                 try:\n",
    "#                     # Find the <a> tag and extract the href attribute\n",
    "#                     download_link = cols[2].find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                    \n",
    "#                     # Handle relative links by appending the base URL if necessary\n",
    "#                     if not download_link.startswith(\"http\"):\n",
    "#                         download_link = f\"https://aiimsnagpur.edu.in{download_link}\"\n",
    "                    \n",
    "#                     # Append the link to the download_links list\n",
    "#                     download_links.append(download_link)\n",
    "                \n",
    "#                 except NoSuchElementException:\n",
    "#                     print(\"No download link found in this row.\")\n",
    "#             else:\n",
    "#                 print(f\"Skipping row with insufficient columns: {len(cols)} columns found.\")\n",
    "\n",
    "#     except NoSuchElementException:\n",
    "#         print(\"No 'Attachments' section found on this page.\")\n",
    "\n",
    "#     return download_links\n",
    "\n",
    "# # Re-select '100 entries' on the main page\n",
    "# def select_100_entries(driver):\n",
    "#     try:\n",
    "#         # Wait for the dropdown to be available and select '100 entries'\n",
    "#         WebDriverWait(driver, 15).until(\n",
    "#             EC.presence_of_element_located((By.NAME, \"archieved_tender_id_length\"))\n",
    "#         )\n",
    "#         select_element = Select(driver.find_element(By.NAME, \"archieved_tender_id_length\"))\n",
    "#         select_element.select_by_value(\"100\")\n",
    "#         time.sleep(3)  # Give the page time to refresh\n",
    "#     except NoSuchElementException:\n",
    "#         print(\"Error: Could not find the '100 entries' dropdown.\")\n",
    "#     except TimeoutException:\n",
    "#         print(\"Error: Timed out waiting for the '100 entries' dropdown.\")\n",
    "\n",
    "# # Check if there is a next page and navigate\n",
    "# def go_to_next_page(driver):\n",
    "#     try:\n",
    "#         # Wait for the \"Next\" button to appear and check if it's enabled\n",
    "#         next_button = WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.ID, \"archieved_tender_id_next\"))\n",
    "#         )\n",
    "#         if \"disabled\" not in next_button.get_attribute(\"class\"):\n",
    "#             next_button.click()\n",
    "#             time.sleep(3)  # Wait for the page to load after clicking\n",
    "#             return True\n",
    "#         else:\n",
    "#             print(\"No more pages available.\")\n",
    "#             return False\n",
    "#     except NoSuchElementException:\n",
    "#         print(\"Next button not found.\")\n",
    "#         return False\n",
    "#     except TimeoutException:\n",
    "#         print(\"Error: Timed out waiting for the 'Next' button.\")\n",
    "#         return False\n",
    "\n",
    "# # Main scraping function with pagination\n",
    "# def scrape_aiims_nagpur_tenders():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Open the URL\n",
    "#     driver.get(\"https://aiimsnagpur.edu.in/tendersfront/archived_tenders\")\n",
    "    \n",
    "#     # Select '100 entries' from the dropdown if necessary\n",
    "#     select_100_entries(driver)\n",
    "\n",
    "#     all_table_data = []\n",
    "\n",
    "#     # Scrape all pages\n",
    "#     while True:\n",
    "#         # Scrape the main table data\n",
    "#         page_data = scrape_main_table(driver)\n",
    "#         all_table_data.extend(page_data)\n",
    "\n",
    "#         # Go to the next page, if available\n",
    "#         if not go_to_next_page(driver):\n",
    "#             break  # Exit the loop when there are no more pages\n",
    "\n",
    "#     # Create a DataFrame using pandas\n",
    "#     if all_table_data:\n",
    "#         df = pd.DataFrame(all_table_data)\n",
    "    \n",
    "#         # Save the DataFrame to an Excel file\n",
    "#         df.to_excel(\"aiims_nagpur_tenders.xlsx\", index=False)\n",
    "    \n",
    "#         print(f\"Data has been successfully scraped and saved to 'aiims_nagpur_tenders.xlsx'.\")\n",
    "#     else:\n",
    "#         print(\"No data found in the table.\")\n",
    "    \n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_aiims_nagpur_tenders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS MANGLAGIRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scrape tenders from a single page\n",
    "# def scrape_tenders_from_page(driver):\n",
    "#     tender_data = []\n",
    "\n",
    "#     try:\n",
    "#         # Wait until the tender elements are present\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.CSS_SELECTOR, \".faq .question\"))\n",
    "#         )\n",
    "\n",
    "#         # Get all the tender blocks (questions)\n",
    "#         tenders = driver.find_elements(By.CSS_SELECTOR, \".faq .question\")\n",
    "\n",
    "#         for tender in tenders:\n",
    "#             try:\n",
    "#                 # Extract the serial number, date, and title\n",
    "#                 serial_number = tender.find_element(By.CSS_SELECTOR, \".num\").text\n",
    "#                 tender_date = tender.find_element(By.CSS_SELECTOR, \".colorbox\").text\n",
    "#                 tender_title = tender.find_element(By.CSS_SELECTOR, \".title\").text.replace(serial_number, '').replace(tender_date, '').strip()\n",
    "\n",
    "#                 # Click to expand the tender to reveal the details\n",
    "#                 tender_title_element = tender.find_element(By.CSS_SELECTOR, \".title\")\n",
    "#                 tender_title_element.click()\n",
    "\n",
    "#                 # Wait for the content to expand\n",
    "#                 time.sleep(2)\n",
    "\n",
    "#                 # Extract the tender details (answer section)\n",
    "#                 answer_element = tender.find_element(By.CSS_SELECTOR, \".answer\")\n",
    "#                 tender_details = answer_element.text.strip()\n",
    "\n",
    "#                 # Extract the download link (if available)\n",
    "#                 try:\n",
    "#                     download_link = answer_element.find_element(By.CSS_SELECTOR, \"a.button\").get_attribute(\"href\")\n",
    "#                 except NoSuchElementException:\n",
    "#                     download_link = None\n",
    "\n",
    "#                 # Print for debugging\n",
    "#                 print(f\"Scraped: {serial_number}, {tender_date}, {tender_title}, Download Link: {download_link}\")\n",
    "\n",
    "#                 # Append the extracted data\n",
    "#                 tender_data.append({\n",
    "#                     \"Serial Number\": serial_number,\n",
    "#                     \"Date\": tender_date,\n",
    "#                     \"Title\": tender_title,\n",
    "#                     \"Details\": tender_details,\n",
    "#                     \"Download Link\": download_link\n",
    "#                 })\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error scraping tender: {e}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading tenders: {e}\")\n",
    "\n",
    "#     return tender_data\n",
    "\n",
    "# # Function to handle pagination and scrape tenders across all pages\n",
    "# def scrape_all_tenders():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Start with the first page\n",
    "#     base_url = \"https://www.aiimsmangalagiri.edu.in/tenders-quotations/tenders/\"\n",
    "#     driver.get(base_url)\n",
    "    \n",
    "#     all_tender_data = []\n",
    "\n",
    "#     while True:\n",
    "#         print(f\"Scraping page: {driver.current_url}\")\n",
    "        \n",
    "#         # Scrape tenders from the current page\n",
    "#         page_tender_data = scrape_tenders_from_page(driver)\n",
    "#         all_tender_data.extend(page_tender_data)\n",
    "\n",
    "#         # Check if there is a \"Next\" button for pagination\n",
    "#         try:\n",
    "#             next_button = driver.find_element(By.LINK_TEXT, \"Next \")\n",
    "#             next_button.click()\n",
    "#             time.sleep(3)  # Wait for the next page to load\n",
    "#         except NoSuchElementException:\n",
    "#             # No \"Next\" button found, meaning we are on the last page\n",
    "#             print(\"Reached the last page.\")\n",
    "#             break\n",
    "\n",
    "#     # Convert the list of tenders to a DataFrame\n",
    "#     df = pd.DataFrame(all_tender_data)\n",
    "\n",
    "#     # Save the DataFrame to an Excel file\n",
    "#     df.to_excel(\"aiims_mangalagiri_tenders.xlsx\", index=False)\n",
    "\n",
    "#     print(\"Data has been successfully scraped and saved to 'aiims_mangalagiri_tenders.xlsx'.\")\n",
    "\n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_all_tenders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS RAIBARELI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException, ElementClickInterceptedException\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scrape tenders from a single page\n",
    "# def scrape_tenders(driver):\n",
    "#     tender_data = []\n",
    "\n",
    "#     try:\n",
    "#         # Wait for the tender elements to load\n",
    "#         time.sleep(3)  # Add a small sleep to allow the page to fully load\n",
    "\n",
    "#         # Get all the tender blocks (card-body)\n",
    "#         tenders = driver.find_elements(By.CSS_SELECTOR, \".col-xl-12 .card-body\")\n",
    "\n",
    "#         for tender in tenders:\n",
    "#             try:\n",
    "#                 # Extract the Ref/Adv Number (Reference number)\n",
    "#                 ref_adv_number = tender.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
    "\n",
    "#                 # Extract the dates (Date of Publishing and Date of Closing)\n",
    "#                 publishing_date = tender.find_elements(By.CSS_SELECTOR, \"a\")[0].text.replace(\"Date of Publishing:\", \"\").strip()\n",
    "#                 closing_date = tender.find_elements(By.CSS_SELECTOR, \"a\")[1].text.replace(\"Date of Closing:\", \"\").strip()\n",
    "\n",
    "#                 # Extract the description\n",
    "#                 description = tender.find_element(By.CSS_SELECTOR, \"p\").text.strip()\n",
    "\n",
    "#                 # Extract the download link (if available)\n",
    "#                 try:\n",
    "#                     download_link = tender.find_element(By.CSS_SELECTOR, \"a.btn\").get_attribute(\"href\")\n",
    "#                 except NoSuchElementException:\n",
    "#                     download_link = None\n",
    "\n",
    "#                 # Print for debugging\n",
    "#                 print(f\"Scraped: Ref No: {ref_adv_number}, Publish Date: {publishing_date}, Closing Date: {closing_date}, Download Link: {download_link}\")\n",
    "\n",
    "#                 # Append the extracted data to the list\n",
    "#                 tender_data.append({\n",
    "#                     \"Reference Number\": ref_adv_number,\n",
    "#                     \"Publish Date\": publishing_date,\n",
    "#                     \"Closing Date\": closing_date,\n",
    "#                     \"Description\": description,\n",
    "#                     \"Download Link\": download_link\n",
    "#                 })\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error scraping tender: {e}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading tenders: {e}\")\n",
    "\n",
    "#     return tender_data\n",
    "\n",
    "# # Function to handle pagination using the \"Next\" button\n",
    "# def scrape_all_tenders():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Start with the first page\n",
    "#     base_url = \"https://aiimsrbl.edu.in/archived/tenders\"\n",
    "#     driver.get(base_url)\n",
    "    \n",
    "#     all_tender_data = []\n",
    "\n",
    "#     while True:\n",
    "#         print(f\"Scraping page: {driver.current_url}\")\n",
    "        \n",
    "#         # Scrape tenders from the current page\n",
    "#         page_tender_data = scrape_tenders(driver)\n",
    "#         all_tender_data.extend(page_tender_data)\n",
    "\n",
    "#         # Check if there is a \"Next\" button for pagination\n",
    "#         try:\n",
    "#             # Wait until the \"Next\" button is clickable, using a higher timeout (10 seconds)\n",
    "#             next_button = WebDriverWait(driver, 10).until(\n",
    "#                 EC.element_to_be_clickable((By.CSS_SELECTOR, \"li.page-next a[rel='next']\"))\n",
    "#             )\n",
    "            \n",
    "#             # Scroll to the \"Next\" button to ensure visibility\n",
    "            \n",
    "#             driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "\n",
    "#             # Try to click the \"Next\" button\n",
    "#             try:\n",
    "#                 next_button.click()\n",
    "#             except ElementClickInterceptedException:\n",
    "#                 # If intercept happens, scroll slightly and retry click using JavaScript\n",
    "#                 print(\"Next button click intercepted. Attempting JavaScript click.\")\n",
    "#                 driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "\n",
    "#             # Wait for the next page to load\n",
    "#             time.sleep(3)\n",
    "        \n",
    "#         except (TimeoutException, NoSuchElementException):\n",
    "#             # If no \"Next\" button is found or the timeout occurs, end the loop\n",
    "#             print(\"Reached the last page or timed out.\")\n",
    "#             break\n",
    "\n",
    "#     # Convert the list of tenders to a DataFrame\n",
    "#     df = pd.DataFrame(all_tender_data)\n",
    "\n",
    "#     # Save the DataFrame to an Excel file\n",
    "#     df.to_excel(\"aiims_raebareli_tenders_with_dates.xlsx\", index=False)\n",
    "\n",
    "#     print(\"Data has been successfully scraped and saved to 'aiims_raebareli_tenders_with_dates.xlsx'.\")\n",
    "\n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_all_tenders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS RISHIKESH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--headless')  # Run in headless mode, remove this line if you want to see the browser window\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scroll to the bottom of the page to handle infinite scroll\n",
    "# def scroll_to_bottom(driver):\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     while True:\n",
    "#         # Scroll down to the bottom\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         # Wait for new content to load\n",
    "#         time.sleep(3)  # Adjust this depending on the page's load time\n",
    "#         # Check if the scroll height has changed\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             break\n",
    "#         last_height = new_height\n",
    "\n",
    "# # Scrape the table content, including extracting links from the \"Tender Title\" column\n",
    "# def scrape_table_data(driver):\n",
    "#     table_data = []\n",
    "    \n",
    "#     # Locate the table rows, wait for table to load\n",
    "#     try:\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.XPATH, \"//table/tbody/tr\"))\n",
    "#         )\n",
    "#     except TimeoutException:\n",
    "#         print(\"Table did not load in time.\")\n",
    "#         return table_data\n",
    "\n",
    "#     rows = driver.find_elements(By.XPATH, \"//table/tbody/tr\")\n",
    "\n",
    "#     for row in rows:\n",
    "#         # Get all the cells (columns) in the row\n",
    "#         cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        \n",
    "#         # Ensure the row has data\n",
    "#         if len(cols) > 0:\n",
    "#             row_data = []\n",
    "#             for i, col in enumerate(cols):\n",
    "#                 # Check if we're in the \"Tender Title\" column (assumed to be column 1 or index 0)\n",
    "#                 if i == 1:  # Assuming Tender Title is the first column, adjust index if different\n",
    "#                     try:\n",
    "#                         # Try to extract the link if there's an anchor tag inside\n",
    "#                         link_elem = col.find_element(By.TAG_NAME, \"a\")\n",
    "#                         title = link_elem.text\n",
    "#                         link = link_elem.get_attribute(\"href\")\n",
    "#                     except NoSuchElementException:\n",
    "#                         # If no link is found, just grab the text and leave the link blank\n",
    "#                         title = col.text\n",
    "#                         link = \"\"\n",
    "                    \n",
    "#                     row_data.append(title)  # Append the title text\n",
    "#                     row_data.append(link)   # Append the link in a new column\n",
    "#                 else:\n",
    "#                     row_data.append(col.text)  # Append the normal text for other columns\n",
    "#             table_data.append(row_data)\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_aiims_tender():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Open the URL\n",
    "#     driver.get(\"https://aiimsrishikesh.edu.in/a1_1/?page_id=2525\")\n",
    "    \n",
    "#     # Scroll to the bottom to ensure all content is loaded (if infinite scroll exists)\n",
    "#     scroll_to_bottom(driver)\n",
    "    \n",
    "#     # Scrape the table\n",
    "#     table_data = scrape_table_data(driver)\n",
    "    \n",
    "#     # Automatically detect the number of columns in the first row\n",
    "#     if len(table_data) > 0:\n",
    "#         num_columns = len(table_data[0])  # Get the number of columns from the first row\n",
    "    \n",
    "#         # Create generic column headers and add one for \"Tender Link\"\n",
    "#         column_headers = ['Tender Title', 'Tender Link'] + [f\"Column {i+2}\" for i in range(num_columns - 2)]\n",
    "    \n",
    "#         # Convert the data into a pandas DataFrame\n",
    "#         df = pd.DataFrame(table_data, columns=column_headers)\n",
    "    \n",
    "#         # Save the DataFrame to an Excel file\n",
    "#         df.to_excel(\"aiims_tender_data_with_links.xlsx\", index=False)\n",
    "    \n",
    "#         print(f\"Data has been successfully scraped and saved to 'aiims_tender_data_with_links.xlsx'.\")\n",
    "    \n",
    "#     else:\n",
    "#         print(\"No data found in the table.\")\n",
    "    \n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_aiims_tender()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS RISHIKESH ARCHIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (1268, 4)\n",
      "Data successfully saved to 'aiims_rishikesh_tenders.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.webdriver.common.by import By\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     options.add_argument('--headless')  # Run in headless mode to avoid opening the browser\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Function to handle infinite scrolling, if required\n",
    "# def scroll_to_bottom(driver):\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     while True:\n",
    "#         # Scroll down to the bottom\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         # Wait for new content to load\n",
    "#         time.sleep(2)  # Adjust the sleep duration if the page is slow\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             break  # If height hasn't changed, we've reached the bottom\n",
    "#         last_height = new_height\n",
    "\n",
    "# # Scrape the table content (only the first 4 columns)\n",
    "# def scrape_tenders_table(driver):\n",
    "#     # Load the webpage\n",
    "#     url = \"https://aiimsrishikesh.edu.in/a1_1/?page_id=2529\"\n",
    "#     driver.get(url)\n",
    "\n",
    "#     # Scroll to the bottom of the page to ensure all content is loaded (if infinite scrolling)\n",
    "#     scroll_to_bottom(driver)\n",
    "\n",
    "#     # Get the page source and pass it to BeautifulSoup\n",
    "#     soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "#     # Find the table\n",
    "#     table = soup.find('table')\n",
    "\n",
    "#     # Extract the table rows\n",
    "#     rows = table.find_all('tr')\n",
    "\n",
    "#     # Initialize a list to store the scraped data\n",
    "#     table_data = []\n",
    "\n",
    "#     # Base URL for the AIIMS Rishikesh site\n",
    "#     base_url = \"https://aiimsrishikesh.edu.in/\"\n",
    "\n",
    "#     # Loop over the rows (skipping the first header row)\n",
    "#     for row in rows[1:]:\n",
    "#         # Get all the cells (columns) in each row\n",
    "#         cells = row.find_all('td')\n",
    "\n",
    "#         # Extract the text from each cell and store it in a list (only the first 4 columns)\n",
    "#         if len(cells) >= 4:\n",
    "#             tender_reference = cells[1].get_text(strip=True)\n",
    "            \n",
    "#             # Extract the link and make sure it's a complete URL\n",
    "#             link_tag = cells[1].find('a')\n",
    "#             if link_tag:\n",
    "#                 description_link = link_tag['href']\n",
    "#                 if not description_link.startswith(\"http\"):\n",
    "#                     description_link = base_url + description_link  # Prepend base URL if link is incomplete\n",
    "#             else:\n",
    "#                 description_link = None\n",
    "\n",
    "#             start_date = cells[2].get_text(strip=True)\n",
    "#             last_date = cells[3].get_text(strip=True)\n",
    "            \n",
    "#             # Append data to the list\n",
    "#             table_data.append([tender_reference, description_link, start_date, last_date])\n",
    "\n",
    "#     return table_data\n",
    "\n",
    "# # Save the scraped data to an Excel file\n",
    "# def save_to_excel(data):\n",
    "#     # Define the column names based on the actual data structure\n",
    "#     columns = ['Tender Reference', 'Download Link', 'Start Date', 'Last Date']\n",
    "    \n",
    "#     # Create a DataFrame with the same number of columns as in the data\n",
    "#     df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "#     # Print the shape of the DataFrame\n",
    "#     print(\"Shape of DataFrame:\", df.shape)\n",
    "\n",
    "#     # Save the DataFrame to an Excel file\n",
    "#     df.to_excel(\"aiims_rishikesh_tenders.xlsx\", index=False)\n",
    "#     print(\"Data successfully saved to 'aiims_rishikesh_tenders.xlsx'\")\n",
    "\n",
    "# # Main function to run the scraper\n",
    "# def main():\n",
    "#     driver = set_up_driver()\n",
    "\n",
    "#     try:\n",
    "#         # Scrape the tenders table\n",
    "#         table_data = scrape_tenders_table(driver)\n",
    "        \n",
    "#         # Save the data to an Excel file\n",
    "#         save_to_excel(table_data)\n",
    "\n",
    "#     finally:\n",
    "#         # Close the browser\n",
    "#         driver.quit()\n",
    "\n",
    "# # Run the main function\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS RAIPUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Function to scrape data from a single page\n",
    "# def scrape_page_data(driver):\n",
    "#     # Get the page source and pass it to BeautifulSoup\n",
    "#     soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "#     # Find the table with the tender data\n",
    "#     table = soup.find('table', class_=\"table table-bordered table-striped nobottommargin\")\n",
    "#     rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "#     # Base URL for the AIIMS Raipur site\n",
    "#     base_url = \"https://www.aiimsraipur.edu.in/\"\n",
    "\n",
    "#     # Initialize a list to store the scraped data for the current page\n",
    "#     page_data = []\n",
    "\n",
    "#     # Loop through all rows and extract data\n",
    "#     for row in rows:\n",
    "#         cells = row.find_all('td')\n",
    "#         if len(cells) >= 6:\n",
    "#             s_no = cells[0].get_text(strip=True)\n",
    "#             tender_no = cells[1].get_text(strip=True)\n",
    "#             title = cells[2].get_text(strip=True)\n",
    "#             publishing_date = cells[3].get_text(strip=True)\n",
    "#             closing_date = cells[4].get_text(strip=True)\n",
    "\n",
    "#             # Extract the download link\n",
    "#             download_tag = cells[5].find('a')\n",
    "#             download_link = base_url + download_tag['href'].lstrip(\"../\") if download_tag else None\n",
    "\n",
    "#             # Extract the corrigendum link\n",
    "#             corrigendum_tag = cells[6].find('a')\n",
    "#             corrigendum_link = base_url + corrigendum_tag['href'].lstrip(\"../\") if corrigendum_tag else None\n",
    "\n",
    "#             # Append the row data to the list\n",
    "#             page_data.append([s_no, tender_no, title, publishing_date, closing_date, download_link, corrigendum_link])\n",
    "\n",
    "#     return page_data\n",
    "\n",
    "# # Function to handle pagination and scrape all pages\n",
    "# def scrape_all_pages(driver):\n",
    "#     all_data = []\n",
    "#     base_url = \"https://www.aiimsraipur.edu.in/user/tenders.php\"\n",
    "#     current_page = 1\n",
    "\n",
    "#     while True:\n",
    "#         # Load the current page\n",
    "#         driver.get(f\"{base_url}?page={current_page}\")\n",
    "#         time.sleep(2)  # Allow time for the page to load\n",
    "\n",
    "#         # Scrape the data from the current page\n",
    "#         page_data = scrape_page_data(driver)\n",
    "#         all_data.extend(page_data)\n",
    "\n",
    "#         # Check if there is a 'Next' button for pagination\n",
    "#         soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#         next_button = soup.find('a', class_=\"pagination\", string=\"Next\")\n",
    "\n",
    "#         if next_button:\n",
    "#             current_page += 1  # Move to the next page\n",
    "#         else:\n",
    "#             break  # Exit the loop if there is no next page\n",
    "\n",
    "#     return all_data\n",
    "\n",
    "# # Save the scraped data to an Excel file\n",
    "# def save_to_excel(data):\n",
    "#     columns = ['S.No', 'Tender No.', 'Title', 'Publishing Date', 'Closing Date', 'Download Link', 'Corrigendum Link']\n",
    "#     df = pd.DataFrame(data, columns=columns)\n",
    "#     df.to_excel(\"aiims_raipur_tenders.xlsx\", index=False)\n",
    "#     print(\"Data successfully saved to 'aiims_raipur_tenders.xlsx'\")\n",
    "\n",
    "# # Main function to run the scraper\n",
    "# def main():\n",
    "#     driver = set_up_driver()\n",
    "\n",
    "#     try:\n",
    "#         # Scrape all the pages\n",
    "#         all_data = scrape_all_pages(driver)\n",
    "        \n",
    "#         # Save the data to an Excel file\n",
    "#         save_to_excel(all_data)\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# # Run the main function\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS PATNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     options.add_argument('--headless')  # Run in headless mode to avoid opening the browser\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Function to scrape data from a single page\n",
    "# def scrape_page_data(driver):\n",
    "#     # Get the page source and pass it to BeautifulSoup\n",
    "#     soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "#     # Find the table with the tender data\n",
    "#     table = soup.find('table', class_=\"MuiTable-root\")\n",
    "#     rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "#     # Initialize a list to store the scraped data for the current page\n",
    "#     page_data = []\n",
    "\n",
    "#     # Loop through all rows and extract data\n",
    "#     for row in rows:\n",
    "#         cells = row.find_all('td')\n",
    "#         if len(cells) >= 6:  # Adjusting for 6 columns\n",
    "#             s_no = cells[0].get_text(strip=True)  # S.No\n",
    "#             tender_no = cells[1].get_text(strip=True)  # Tender No.\n",
    "#             name = cells[2].get_text(strip=True)  # Name\n",
    "#             publishing_date = cells[3].get_text(strip=True)  # Publishing Date\n",
    "#             closing_date = cells[5].get_text(strip=True)  # Closing Date\n",
    "\n",
    "#             # Extract the link from the 6th column (assuming the link is inside an <a> tag)\n",
    "#             link_tag = cells[3].find('a')\n",
    "#             link = link_tag['href'] if link_tag else ''  # If there is no link, set it to an empty string\n",
    "\n",
    "#             # Append the row data to the list\n",
    "#             page_data.append([s_no, tender_no, name, publishing_date, closing_date, link])\n",
    "\n",
    "#     return page_data\n",
    "\n",
    "# # Function to handle pagination and scrape all pages\n",
    "# def scrape_all_pages(driver):\n",
    "#     all_data = []\n",
    "#     base_url = \"https://aiimspatna.edu.in/tender/\"\n",
    "#     current_page = 1\n",
    "\n",
    "#     while True:\n",
    "#         # Load the current page\n",
    "#         driver.get(f\"{base_url}?page={current_page}\")\n",
    "#         time.sleep(2)  # Allow time for the page to load\n",
    "\n",
    "#         # Scrape the data from the current page\n",
    "#         page_data = scrape_page_data(driver)\n",
    "#         all_data.extend(page_data)\n",
    "\n",
    "#         # Check if there is a 'Next' button for pagination and if it is disabled\n",
    "#         soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#         next_button = soup.find('span', title=\"Next Page\").find('button')\n",
    "        \n",
    "#         if 'Mui-disabled' in next_button.get('class', []):  # If the 'Next' button is disabled\n",
    "#             break  # Exit the loop if there is no next page\n",
    "\n",
    "#         current_page += 1  # Move to the next page\n",
    "\n",
    "#     return all_data\n",
    "\n",
    "# # Save the scraped data to an Excel file\n",
    "# def save_to_excel(data):\n",
    "#     columns = ['S.No', 'Tender No.', 'Name', 'Publishing Date', 'Closing Date', 'Link']\n",
    "#     df = pd.DataFrame(data, columns=columns)\n",
    "#     df.to_excel(\"aiims_patna_tenders.xlsx\", index=False)\n",
    "#     print(\"Data successfully saved to 'aiims_patna_tenders.xlsx'\")\n",
    "\n",
    "# # Main function to run the scraper\n",
    "# def main():\n",
    "#     driver = set_up_driver()\n",
    "\n",
    "#     try:\n",
    "#         # Scrape all the pages\n",
    "#         all_data = scrape_all_pages(driver)\n",
    "        \n",
    "#         # Save the data to an Excel file\n",
    "#         save_to_excel(all_data)\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# # Run the main function\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  AIIMS JODHPUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    " \n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scroll to the bottom of the page to handle infinite scroll\n",
    "# def scroll_to_bottom(driver):\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     while True:\n",
    "#         # Scroll down to the bottom\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         # Wait for new content to load\n",
    "#         time.sleep(3)  # Adjust this depending on the page's load time\n",
    "#         # Check if the scroll height has changed\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             break\n",
    "#         last_height = new_height\n",
    "\n",
    "# # Scrape the table content, including extracting links from the \"Tender Title\" column\n",
    "# def scrape_table_data(driver):\n",
    "#     table_data = []\n",
    "    \n",
    "#     # Locate the table rows, wait for table to load\n",
    "#     try:\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.XPATH, \"//table/tbody/tr\"))\n",
    "#         )\n",
    "#     except TimeoutException:\n",
    "#         print(\"Table did not load in time.\")\n",
    "#         return table_data\n",
    "\n",
    "#     rows = driver.find_elements(By.XPATH, \"//table/tbody/tr\")\n",
    "\n",
    "#     for row in rows:\n",
    "#         # Get all the cells (columns) in the row\n",
    "#         cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        \n",
    "#         # Ensure the row has data\n",
    "#         if len(cols) > 0:\n",
    "#             row_data = []\n",
    "#             for i, col in enumerate(cols):\n",
    "#                 # Check if we're in the \"Tender Title\" column (assumed to be column 1 or index 0)\n",
    "#                 if i == 1:  # Assuming Tender Title is the first column, adjust index if different\n",
    "#                     try:\n",
    "#                         # Try to extract the link if there's an anchor tag inside\n",
    "#                         link_elem = col.find_element(By.TAG_NAME, \"a\")\n",
    "#                         title = link_elem.text\n",
    "#                         link = link_elem.get_attribute(\"href\")\n",
    "#                     except NoSuchElementException:\n",
    "#                         # If no link is found, just grab the text and leave the link blank\n",
    "#                         title = col.text\n",
    "#                         link = \"\"\n",
    "                    \n",
    "#                     row_data.append(title)  # Append the title text\n",
    "#                     row_data.append(link)   # Append the link in a new column\n",
    "#                 else:\n",
    "#                     row_data.append(col.text)  # Append the normal text for other columns\n",
    "#             table_data.append(row_data)\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_aiims_tender():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Open the URL\n",
    "#     driver.get(\"https://www.aiimsjodhpur.edu.in/tendernew.php\")\n",
    "    \n",
    "#     # Scroll to the bottom to ensure all content is loaded (if infinite scroll exists)\n",
    "#     scroll_to_bottom(driver)\n",
    "    \n",
    "#     # Scrape the table\n",
    "#     table_data = scrape_table_data(driver)\n",
    "    \n",
    "#     # Automatically detect the number of columns in the first row\n",
    "#     if len(table_data) > 0:\n",
    "#         num_columns = len(table_data[0])  # Get the number of columns from the first row\n",
    "    \n",
    "#         # Create generic column headers and add one for \"Tender Link\"\n",
    "#         column_headers = ['Tender Title', 'Tender Link'] + [f\"Column {i+2}\" for i in range(num_columns - 2)]\n",
    "    \n",
    "#         # Convert the data into a pandas DataFrame\n",
    "#         df = pd.DataFrame(table_data, columns=column_headers)\n",
    "    \n",
    "#         # Save the DataFrame to an Excel file\n",
    "#         df.to_excel(\"aiims_tender_data_with_links.xlsx\", index=False)\n",
    "    \n",
    "#         print(f\"Data has been successfully scraped and saved to 'aiims_tender_data_with_links.xlsx'.\")\n",
    "    \n",
    "#     else:\n",
    "#         print(\"No data found in the table.\")\n",
    "    \n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_aiims_tender()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS BHUBHNESHWAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import time\n",
    "# import pandas as pd\n",
    "\n",
    "# # Initialize WebDriver (automatically download if needed)\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# # Navigate to the page\n",
    "# url = \"https://aiimsbhubaneswar.nic.in/tender.aspx\"\n",
    "# driver.get(url)\n",
    "\n",
    "# def extract_table_data():\n",
    "#     \"\"\"Extract data from the current table on the page.\"\"\"\n",
    "#     table_data = []\n",
    "\n",
    "#     # Locate the table body\n",
    "#     table_rows = driver.find_elements(By.XPATH, \"//div[@class='table-responsive']//tbody/tr\")\n",
    "\n",
    "#     for row in table_rows:\n",
    "#         # Extract all the columns in the row\n",
    "#         cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "#         if len(cols) == 8:\n",
    "#             data = {\n",
    "#                 \"S.N.\": cols[0].text.strip(),\n",
    "#                 \"Title\": cols[1].text.strip(),\n",
    "#                 \"Publishing Date\": cols[2].text.strip(),\n",
    "#                 \"Closing Date\": cols[3].text.strip(),\n",
    "#                 \"Download Link\": cols[4].find_element(By.TAG_NAME, \"a\").get_attribute(\"href\") if cols[4].find_elements(By.TAG_NAME, \"a\") else None,\n",
    "#                 \"Corrigendum\": cols[5].text.strip(),\n",
    "#                 \"Cancellation\": cols[6].text.strip(),\n",
    "#                 \"Award of Contract\": cols[7].text.strip()\n",
    "#             }\n",
    "#             table_data.append(data)\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# def click_next_page():\n",
    "#     \"\"\"Click the next button if available, and return False if no next button.\"\"\"\n",
    "#     try:\n",
    "#         next_button = driver.find_element(By.ID, \"ctl00_ContentPlaceHolder1_lnkNext\")\n",
    "#         next_button.click()\n",
    "#         time.sleep(3)  # Allow time for the new page to load\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# # List to store all scraped data\n",
    "# all_table_data = []\n",
    "\n",
    "# # Scrape data from the first page\n",
    "# all_table_data.extend(extract_table_data())\n",
    "\n",
    "# # Handle pagination and scrape subsequent pages\n",
    "# while click_next_page():\n",
    "#     all_table_data.extend(extract_table_data())\n",
    "\n",
    "# # Close the driver\n",
    "# driver.quit()\n",
    "\n",
    "# # Convert the data into a pandas DataFrame for easier manipulation\n",
    "# df = pd.DataFrame(all_table_data)\n",
    "\n",
    "# # Save to Excel\n",
    "# excel_filename = 'tender_data.xlsx'\n",
    "# df.to_excel(excel_filename, index=False)\n",
    "\n",
    "# print(f\"Data saved to {excel_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS BHUBNESHWAR ARCHIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import time\n",
    "# import pandas as pd\n",
    "\n",
    "# # Initialize WebDriver (automatically download if needed)\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# # Navigate to the page\n",
    "# url = \"https://aiimsbhubaneswar.nic.in/Tender_Archive.aspx\"\n",
    "# driver.get(url)\n",
    "\n",
    "# def extract_table_data():\n",
    "#     \"\"\"Extract data from the current table on the page.\"\"\"\n",
    "#     table_data = []\n",
    "\n",
    "#     # Locate the table body\n",
    "#     table_rows = driver.find_elements(By.XPATH, \"//div[@class='table-responsive']//tbody/tr\")\n",
    "\n",
    "#     for row in table_rows:\n",
    "#         # Extract all the columns in the row\n",
    "#         cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "#         if len(cols) == 8:\n",
    "#             data = {\n",
    "#                 \"S.N.\": cols[0].text.strip(),\n",
    "#                 \"Title\": cols[1].text.strip(),\n",
    "#                 \"Publishing Date\": cols[2].text.strip(),\n",
    "#                 \"Closing Date\": cols[3].text.strip(),\n",
    "#                 \"Download Link\": cols[4].find_element(By.TAG_NAME, \"a\").get_attribute(\"href\") if cols[4].find_elements(By.TAG_NAME, \"a\") else None,\n",
    "#                 \"Corrigendum\": cols[5].text.strip(),\n",
    "#                 \"Cancellation\": cols[6].text.strip(),\n",
    "#                 \"Award of Contract\": cols[7].text.strip()\n",
    "#             }\n",
    "#             table_data.append(data)\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# def click_next_page():\n",
    "#     \"\"\"Click the next button if available, and return False if no next button.\"\"\"\n",
    "#     try:\n",
    "#         next_button = driver.find_element(By.ID, \"ctl00_ContentPlaceHolder1_lnkNext\")\n",
    "#         next_button.click()\n",
    "#         time.sleep(3)  # Allow time for the new page to load\n",
    "#         return True\n",
    "#     except:\n",
    "#         return False\n",
    "\n",
    "# # List to store all scraped data\n",
    "# all_table_data = []\n",
    "\n",
    "# # Scrape data from the first page\n",
    "# all_table_data.extend(extract_table_data())\n",
    "\n",
    "# # Handle pagination and scrape subsequent pages\n",
    "# while click_next_page():\n",
    "#     all_table_data.extend(extract_table_data())\n",
    "\n",
    "# # Close the driver\n",
    "# driver.quit()\n",
    "\n",
    "# # Convert the data into a pandas DataFrame for easier manipulation\n",
    "# df = pd.DataFrame(all_table_data)\n",
    "\n",
    "# # Save to Excel\n",
    "# excel_filename = 'tender_data.xlsx'\n",
    "# df.to_excel(excel_filename, index=False)\n",
    "\n",
    "# print(f\"Data saved to {excel_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS DELHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException\n",
    "\n",
    "# # Set up the Chrome WebDriver\n",
    "# def set_up_driver():\n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('--no-sandbox')\n",
    "#     options.add_argument('--disable-dev-shm-usage')\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "#     return driver\n",
    "\n",
    "# # Scrape the main table content from the AIIMS tenders page\n",
    "# def scrape_tender_table(driver):\n",
    "#     table_data = []\n",
    "\n",
    "#     try:\n",
    "#         # Wait for the table to load\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.CLASS_NAME, \"aiimstender\"))\n",
    "#         )\n",
    "\n",
    "#         # Locate the table body containing the rows\n",
    "#         table_body = driver.find_element(By.CSS_SELECTOR, \".aiimstender tbody\")\n",
    "#         rows = table_body.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "#         for row in rows:\n",
    "#             # Get all the cells (columns) in the row\n",
    "#             cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "\n",
    "#             # Ensure the row has data\n",
    "#             if len(cols) > 0:\n",
    "#                 try:\n",
    "#                     # Extract Tender Title and its embedded link\n",
    "#                     title_element = cols[2].find_element(By.TAG_NAME, \"a\")\n",
    "#                     tender_title = title_element.text\n",
    "#                     tender_link = title_element.get_attribute(\"href\")\n",
    "\n",
    "#                     # Ensure that the link is an absolute URL, otherwise prepend the base URL\n",
    "#                     if not tender_link.startswith(\"http\"):\n",
    "#                         tender_link = \"https://www.aiims.edu\" + tender_link\n",
    "#                 except NoSuchElementException:\n",
    "#                     tender_title = \"\"\n",
    "#                     tender_link = \"\"\n",
    "\n",
    "#                 # Extract S. No, Tender Category, Date of Publishing, and Date of Closing\n",
    "#                 s_no = cols[0].text\n",
    "#                 tender_category = cols[1].text\n",
    "#                 date_publishing = cols[3].text\n",
    "#                 date_closing = cols[4].text\n",
    "\n",
    "#                 # Append the extracted data to table_data\n",
    "#                 table_data.append({\n",
    "#                     \"S. No\": s_no,\n",
    "#                     \"Tender Category\": tender_category,\n",
    "#                     \"Tender Title\": tender_title,\n",
    "#                     \"Date of Publishing\": date_publishing,\n",
    "#                     \"Date of Closing\": date_closing,\n",
    "#                     \"Tender Link\": tender_link\n",
    "#                 })\n",
    "\n",
    "#                 # Print the data for debugging\n",
    "#                 print(f\"Scraped: S. No: {s_no}, Tender Category: {tender_category}, Tender Title: {tender_title}, Date of Publishing: {date_publishing}, Date of Closing: {date_closing}, Link: {tender_link}\")\n",
    "\n",
    "#     except TimeoutException:\n",
    "#         print(\"Error: Timed out waiting for the table to load.\")\n",
    "#     except NoSuchElementException:\n",
    "#         print(\"Error: Could not find the table on the page.\")\n",
    "    \n",
    "#     return table_data\n",
    "\n",
    "# # Scrape the detailed page for each tender\n",
    "# def scrape_tender_details(driver, tender_url):\n",
    "#     tender_details = {}\n",
    "\n",
    "#     try:\n",
    "#         # Navigate to the tender page\n",
    "#         driver.get(tender_url)\n",
    "\n",
    "#         # Wait until the detail section is loaded\n",
    "#         WebDriverWait(driver, 10).until(\n",
    "#             EC.presence_of_element_located((By.CLASS_NAME, \"detail\"))\n",
    "#         )\n",
    "\n",
    "#         # Initialize variables for tender information\n",
    "#         tender_no, title, date_uploading, date_opening, document_link, document_img_src = \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "#         # Find all \"div.detail\" sections\n",
    "#         details = driver.find_elements(By.CLASS_NAME, \"detail\")\n",
    "\n",
    "#         # Iterate over all details to find the required info\n",
    "#         for detail in details:\n",
    "#             try:\n",
    "#                 left_element = detail.find_element(By.CLASS_NAME, \"left\").text\n",
    "#                 right_element = detail.find_element(By.CLASS_NAME, \"right\")\n",
    "\n",
    "#                 # Extract the Tender No\n",
    "#                 if left_element == \"Tender No\":\n",
    "#                     tender_no = right_element.text\n",
    "\n",
    "#                 # Extract the Title\n",
    "#                 elif left_element == \"Title\":\n",
    "#                     title = right_element.text\n",
    "\n",
    "#                 # Extract Date of Uploading\n",
    "#                 elif left_element == \"Date of Uploading\":\n",
    "#                     date_uploading = right_element.text\n",
    "\n",
    "#                 # Extract Date of Opening\n",
    "#                 elif left_element == \"Date of Opening\":\n",
    "#                     date_opening = right_element.text\n",
    "\n",
    "#                 # Extract the Document link and Image src if present\n",
    "#                 elif left_element == \"Document\":\n",
    "#                     try:\n",
    "#                         # Find the <a> tag in the \"Document\" section and extract the link\n",
    "#                         document_element = right_element.find_element(By.XPATH, \"//*[@id='zt-component']/article/div/div[10]/a\")\n",
    "#                         document_link = document_element.get_attribute(\"href\")\n",
    "\n",
    "#                         # Extract the <img> tag inside <a> and get its src\n",
    "#                         document_img = document_element.find_element(By.TAG_NAME, \"img\")\n",
    "#                         document_img_src = document_img.get_attribute(\"src\")\n",
    "\n",
    "#                         # Handle relative links for both href and src\n",
    "#                         if not document_link.startswith(\"http\"):\n",
    "#                             document_link = \"https://www.aiims.edu\" + document_link\n",
    "#                         if not document_img_src.startswith(\"http\"):\n",
    "#                             document_img_src = \"https://www.aiims.edu\" + document_img_src\n",
    "\n",
    "#                     except NoSuchElementException:\n",
    "#                         print(f\"No document link or image found for tender at {tender_url}\")\n",
    "\n",
    "#             except NoSuchElementException:\n",
    "#                 print(\"Some details could not be extracted from this section.\")\n",
    "\n",
    "#         # Add the details to the dictionary\n",
    "#         tender_details = {\n",
    "#             \"Tender No\": tender_no,\n",
    "#             \"Title\": title,\n",
    "#             \"Date of Uploading\": date_uploading,\n",
    "#             \"Date of Opening\": date_opening,\n",
    "#             \"Document Link\": document_link,\n",
    "#             \"Document Image Src\": document_img_src  # Store the document image source\n",
    "#         }\n",
    "\n",
    "#     except TimeoutException:\n",
    "#         print(f\"Error: Timed out waiting for details page at {tender_url}\")\n",
    "#     except NoSuchElementException:\n",
    "#         print(f\"Error: Could not find details section on page {tender_url}\")\n",
    "#     except WebDriverException as e:\n",
    "#         print(f\"Error navigating to {tender_url}: {e}\")\n",
    "    \n",
    "#     return tender_details\n",
    "\n",
    "# # Main function to scrape AIIMS tenders and details\n",
    "# def scrape_aiims_tenders():\n",
    "#     driver = set_up_driver()\n",
    "    \n",
    "#     # Open the AIIMS tenders URL\n",
    "#     driver.get(\"https://www.aiims.edu/index.php/en/tenders/aiims-tender\")\n",
    "    \n",
    "#     # Scrape the tender table\n",
    "#     tender_data = scrape_tender_table(driver)\n",
    "    \n",
    "#     # Scrape details for each tender and update the data\n",
    "#     for tender in tender_data:\n",
    "#         print(f\"Navigating to: {tender['Tender Title']} at {tender['Tender Link']}\")\n",
    "        \n",
    "#         try:\n",
    "#             # Scrape details for the specific tender\n",
    "#             details = scrape_tender_details(driver, tender['Tender Link'])\n",
    "\n",
    "#             # Update the tender data with the scraped details\n",
    "#             tender.update(details)\n",
    "\n",
    "#             # Print scraped details for debugging\n",
    "#             print(f\"Scraped details: {details}\")\n",
    "\n",
    "#         except WebDriverException as e:\n",
    "#             # Handle navigation or scraping errors\n",
    "#             print(f\"Failed to scrape {tender['Tender Title']} due to an error: {e}\")\n",
    "\n",
    "#         finally:\n",
    "#             # Navigate back to the main page to continue processing other tenders\n",
    "#             driver.back()\n",
    "#             time.sleep(3)  # Give time for the page to load before continuing\n",
    "    \n",
    "#     # Create a DataFrame using pandas\n",
    "#     if tender_data:\n",
    "#         df = pd.DataFrame(tender_data)\n",
    "    \n",
    "#         # Save the DataFrame to an Excel file\n",
    "#         df.to_excel(\"aiims_tenders_with_details.xlsx\", index=False)\n",
    "    \n",
    "#         print(f\"Data has been successfully scraped and saved to 'aiims_tenders_with_details.xlsx'.\")\n",
    "#     else:\n",
    "#         print(\"No data found in the table.\")\n",
    "    \n",
    "#     # Close the browser\n",
    "#     driver.quit()\n",
    "\n",
    "# # Run the scraper\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_aiims_tenders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS BHOPAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# import time\n",
    "\n",
    "# # Set up the WebDriver (this example uses Chrome)\n",
    "# driver_path = 'path/to/chromedriver'  # <-- Replace this with your WebDriver path\n",
    "# service = Service(driver_path)\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# # Open the webpage\n",
    "# url = 'https://aiimsbhopal.edu.in/index_controller/manualTender#firstLink2'\n",
    "# driver.get(url)\n",
    "\n",
    "# # Wait for the page to load completely using WebDriverWait\n",
    "# WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'tableborder')))\n",
    "\n",
    "# # Find all collapsible panels (we assume these are the sections you want to expand)\n",
    "# panels = driver.find_elements(By.CSS_SELECTOR, 'li.panel')\n",
    "\n",
    "# # Loop through each panel and click to expand (if they need clicking)\n",
    "# for panel in panels:\n",
    "#     try:\n",
    "#         # Scroll to the element before clicking to ensure its visible\n",
    "#         driver.execute_script(\"arguments[0].scrollIntoView(true);\", panel)\n",
    "\n",
    "#         # Check if the panel has a clickable element (such as an <a> tag)\n",
    "#         panel_header = panel.find_element(By.TAG_NAME, 'a')\n",
    "        \n",
    "#         # Use JavaScript to click the element to avoid interception issues\n",
    "#         driver.execute_script(\"arguments[0].click();\", panel_header)\n",
    "#         time.sleep(2)  # Small delay to allow content to load\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error expanding panel: {e}\")\n",
    "\n",
    "# # Parse the updated page content using BeautifulSoup after expanding panels\n",
    "# soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# # Initialize a list to collect all scraped tables\n",
    "# all_tables_data = []\n",
    "\n",
    "# # Extract all tables (including the initial one and those in expanded sections)\n",
    "# tables = soup.find_all('table', {'class': 'table table-bordered table-hover table-responsive table-striped tableborder'})\n",
    "\n",
    "# # Loop through each table and extract data\n",
    "# for table in tables:\n",
    "#     table_data = []\n",
    "#     rows = table.find_all('tr')\n",
    "\n",
    "#     for row in rows:\n",
    "#         cols = row.find_all(['td', 'th'])\n",
    "#         row_data = []\n",
    "#         for col in cols:\n",
    "#             link = col.find('a')  # Check if there's a link in the cell\n",
    "#             if link:\n",
    "#                 row_data.append(f\"{col.text.strip()} (Link: {link.get('href')})\")\n",
    "#             else:\n",
    "#                 row_data.append(col.text.strip())\n",
    "#         table_data.append(row_data)\n",
    "\n",
    "#     # Append table data to the all_tables_data list\n",
    "#     all_tables_data.append(table_data)\n",
    "\n",
    "# # Flatten all collected table data into a Pandas DataFrame\n",
    "# flattened_data = [row for table in all_tables_data for row in table]\n",
    "# df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# # Save the DataFrame to an Excel file\n",
    "# df.to_excel('expanded_tender_data.xlsx', index=False, header=False)\n",
    "\n",
    "# print(\"Data saved to expanded_tender_data.xlsx\")\n",
    "\n",
    "# # Close the WebDriver\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIMS DEOGHAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "# from selenium.webdriver.common.by import By\n",
    "# import time\n",
    "# import pandas as pd\n",
    "\n",
    "# # Set up the webdriver (using Chrome as an example)\n",
    "# driver = webdriver.Chrome()\n",
    "\n",
    "# # Step 1: Open the target URL\n",
    "# driver.get(\"https://www.aiimsdeoghar.edu.in/tender/list\")\n",
    "\n",
    "# # Allow the page to load\n",
    "# time.sleep(3)\n",
    "\n",
    "# # Step 2: Find the dropdown menu by its name attribute and select '100'\n",
    "# dropdown = Select(driver.find_element(By.NAME, \"table_id_length\"))\n",
    "# dropdown.select_by_value(\"100\")\n",
    "\n",
    "# # Allow the table to reload with 100 entries\n",
    "# time.sleep(2)\n",
    "\n",
    "# # Step 3: Locate the table element\n",
    "# table = driver.find_element(By.ID, \"table_id\")\n",
    "\n",
    "# # Step 4: Extract the table rows\n",
    "# rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "# # Step 5: Scrape up to 5 columns of data from each row\n",
    "# data = []\n",
    "# for row in rows[1:]:  # Skip the header row\n",
    "#     columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "#     if len(columns) >= 5:\n",
    "#         s_no = columns[0].text\n",
    "#         advertisement_no = columns[1].text\n",
    "#         title = columns[2].text\n",
    "#         publish_date = columns[3].text\n",
    "#         download_link = columns[4].find_element(By.TAG_NAME, 'a').get_attribute('href')  # Get download link\n",
    "        \n",
    "#         data.append([s_no, advertisement_no, title, publish_date, download_link])\n",
    "\n",
    "# # Step 6: Convert the scraped data to a Pandas DataFrame (optional, for convenience)\n",
    "# df = pd.DataFrame(data, columns=[\"SNo\", \"Advertisement No.\", \"Title\", \"Publish Date\", \"Download Link\"])\n",
    "\n",
    "# # Step 7: Print or save the DataFrame\n",
    "# print(df)\n",
    "\n",
    "# # Optionally, save to CSV\n",
    "# df.to_csv(\"aiims_tender_data.csv\", index=False)\n",
    "\n",
    "# # Close the browser\n",
    "# driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
